# Deep Learning for Visual Tracking: A Comprehensive Survey
The comprehensive comparisons of recent Deep Learning (DL)-based visual tracking methods on the OTB-2013, OTB-2018, VOT-2018, and LaSOT datasets

## Performance Comparison of Visual Trackers in terms of Precision and Success Plots on OTB-2013 Dataset (Ranking Based on Area under Curve (AUC)) [[OTB-2013 Dataset]](http://cvlab.hanyang.ac.kr/tracker_benchmark/datasets.html) [[OTB-2013 Paper]](https://ieeexplore.ieee.org/document/6619156): 
**- Average Performance Comparisons of Visual Tracking Methods:**
<img src="OTB2013%20results/1.png"/> 
<img src="OTB2013%20results/13.png"/>
**- Attribute-based Performance Comparisons (Eleven attributes including: Illumination Variation (IV), Scale Variation (SV), Occlusion (OCC), Deformation (DEF), Motion Blur (MB), Fast Motion (FM), In-Plane Rotation (IPR), Out-of-Plane Rotation (OPR), Out-of-View (OV), Background Clutter (BC), Low Resolution (LR)):**
<img src="OTB2013%20results/2.png"/>
<img src="OTB2013%20results/14.png"/>
<img src="OTB2013%20results/3.png"/>
<img src="OTB2013%20results/15.png"/>
<img src="OTB2013%20results/4.png" />
<img src="OTB2013%20results/16.png"/>
<img src="OTB2013%20results/5.png"/>
<img src="OTB2013%20results/17.png"/>
<img src="OTB2013%20results/6.png"/>
<img src="OTB2013%20results/18.png"/>
<img src="OTB2013%20results/7.png"/>
<img src="OTB2013%20results/19.png"/>
<img src="OTB2013%20results/8.png"/>
<img src="OTB2013%20results/20.png"/>
<img src="OTB2013%20results/9.png"/>
<img src="OTB2013%20results/21.png"/>
<img src="OTB2013%20results/10.png"/>
<img src="OTB2013%20results/22.png"/>
<img src="OTB2013%20results/11.png"/>
<img src="OTB2013%20results/23.png"/>
<img src="OTB2013%20results/12.png"/>
<img src="OTB2013%20results/24.png"/>

## Performance Comparison of Visual Trackers in terms of Precision and Success Plots on OTB-2015 Dataset (Ranking Based on Area under Curve (AUC)) [[OTB-2015 Dataset]](http://cvlab.hanyang.ac.kr/tracker_benchmark/datasets.html)[[OTB-2015 Paper]](https://ieeexplore.ieee.org/document/7001050):
**- Average Performance Comparisons of Visual Tracking Methods:**
<img src="OTB2015%20results/1.png"/> 
<img src="OTB2015%20results/13.png"/>
**- Attribute-based Performance Comparisons (Eleven attributes including: Illumination Variation (IV), Scale Variation (SV), Occlusion (OCC), Deformation (DEF), Motion Blur (MB), Fast Motion (FM), In-Plane Rotation (IPR), Out-of-Plane Rotation (OPR), Out-of-View (OV), Background Clutter (BC), Low Resolution (LR)):**
<img src="OTB2015%20results/2.png"/>
<img src="OTB2015%20results/14.png"/>
<img src="OTB2015%20results/3.png"/>
<img src="OTB2015%20results/15.png"/>
<img src="OTB2015%20results/4.png" />
<img src="OTB2015%20results/16.png"/>
<img src="OTB2015%20results/5.png"/>
<img src="OTB2015%20results/17.png"/>
<img src="OTB2015%20results/6.png"/>
<img src="OTB2015%20results/18.png"/>
<img src="OTB2015%20results/7.png"/>
<img src="OTB2015%20results/19.png"/>
<img src="OTB2015%20results/8.png"/>
<img src="OTB2015%20results/20.png"/>
<img src="OTB2015%20results/9.png"/>
<img src="OTB2015%20results/21.png"/>
<img src="OTB2015%20results/10.png"/>
<img src="OTB2015%20results/22.png"/>
<img src="OTB2015%20results/11.png"/>
<img src="OTB2015%20results/23.png"/>
<img src="OTB2015%20results/12.png"/>
<img src="OTB2015%20results/24.png"/>

##  Performance Comparison of Visual Trackers on VOT-2018 Dataset [[VOT-2018 Dataset]](http://www.votchallenge.net/vot2018/dataset.html)[[VOT-2018 Paper]](https://link.springer.com/chapter/10.1007/978-3-030-11009-3_1):
AR plot for camera motion in experiment baseline (Accuracy-Robustness: Experiment baseline)
<img src="VOT2018%20results/AR plot for camera motion in experiment baseline_Accuracy-Robustness_Experiment baseline.png"/> 

AR plot for experiment baseline (mean) (Accuracy-Robustness: Experiment baseline)
<img src="VOT2018%20results/AR plot for experiment baseline (mean)_Accuracy-Robustness_Experiment baseline.png"/>

AR plot for experiment baseline (pooled) (Accuracy-Robustness: Experiment baseline)
<img src="VOT2018%20results/AR plot for experiment baseline (pooled)_Accuracy-Robustness_Experiment baseline.png"/>

AR plot for experiment baseline (weighted_mean) (Accuracy-Robustness: Experiment baseline)
<img src="VOT2018%20results/AR plot for experiment baseline (weighted_mean)_Accuracy-Robustness_Experiment baseline.png"/>

AR plot for illumination change in experiment baseline (Accuracy-Robustness: Experiment baseline)
<img src="VOT2018%20results/AR plot for illumination change in experiment baseline_Accuracy-Robustness_Experiment baseline.png"/>

AR plot for motion change in experiment baseline (Accuracy-Robustness: Experiment baseline)
<img src="VOT2018%20results/AR plot for motion change in experiment baseline_Accuracy-Robustness_Experiment baseline.png"/>

AR plot for no degradation in experiment baseline (Accuracy-Robustness: Experiment baseline)
<img src="VOT2018%20results/AR plot for no degradation in experiment baseline_Accuracy-Robustness_Experiment baseline.png" />

AR plot for occlusion in experiment baseline (Accuracy-Robustness: Experiment baseline)
<img src="VOT2018%20results/AR plot for occlusion in experiment baseline_Accuracy-Robustness_Experiment baseline.png"/>

AR plot for size change in experiment baseline (Accuracy-Robustness: Experiment baseline)
<img src="VOT2018%20results/AR plot for size change in experiment baseline_Accuracy-Robustness_Experiment baseline.png"/>

Expected overlap curves for baseline (Expected overlap analysis: Experiment baseline)
<img src="VOT2018%20results/Expected overlap curves for baseline_Expected overlap analysis_Experiment baseline.png"/>

Expected overlap scores for baseline (Expected overlap analysis: Experiment baseline)
<img src="VOT2018%20results/Expected overlap scores for baseline_Expected overlap analysis_Experiment baseline.png"/>

Experiment unsupervised (average) (Experiment unsupervised)
<img src="VOT2018%20results/Experiment unsupervised (average)_Experiment unsupervised.png"/>

Orderings for failures (Accuracy-Robustness: Experiment baseline)
<img src="VOT2018%20results/Orderings for failures_Accuracy-Robustness_Experiment baseline.png"/>

Orderings for overall overlap (Accuracy-Robustness: Experiment baseline)
<img src="VOT2018%20results/Orderings for overall overlap_Accuracy-Robustness_Experiment baseline.png"/>

Orderings for overall overlap (Experiment unsupervised)
<img src="VOT2018%20results/Orderings for overall overlap_Experiment unsupervised.png"/>

Overlap plot for camera motion in experiment unsupervised
<img src="VOT2018%20results/Overlap plot for tag tag_camera_motion in experiment unsupervised.png"/>

Overlap plot for no degradation in experiment unsupervised
<img src="VOT2018%20results/Overlap plot for tag tag_empty in experiment unsupervised.png"/>

Overlap plot for illum change in experiment unsupervised
<img src="VOT2018%20results/Overlap plot for tag tag_illum_change in experiment unsupervised.png"/>

Overlap plot for motion change in experiment unsupervised
<img src="VOT2018%20results/Overlap plot for tag tag_motion_change in experiment unsupervised.png"/>

Overlap plot for occlusion in experiment unsupervised
<img src="VOT2018%20results/Overlap plot for tag tag_occlusion in experiment unsupervised.png"/>

Overlap plot for size change in experiment unsupervised
<img src="VOT2018%20results/Overlap plot for tag tag_size_change in experiment unsupervised.png"/>

Table Accuracy (Accuracy-Robustness: Experiment Baseline)
<img src="VOT2018%20results/Accuracy Table Accuracy-Robustness Experiment Baseline.png"/>

Table Robustness (Accuracy-Robustness: Experiment Baseline)
<img src="VOT2018%20results/Robustness Table Accuracy-Robustness Experiment Baseline.png"/>

Report Overview
<img src="VOT2018%20results/Report Overview.png"/>

Overlap Overview Experiment Unsupervised
<img src="VOT2018%20results/Overlap Overview Experiment Unsupervised.png"/>

Overview Expected Overlap Analysis Experiment Analysis
<img src="VOT2018%20results/Overview Expected Overlap Analysis Experiment Analysis.png"/>

## Qualitative Comparisons of State-of-the-art Visual Tracking Methods on VOT2018 Dataset (Under [TraX Protocol] (https://www.sciencedirect.com/science/article/pii/S0925231217303065)):
[BMX_Video](https://www.youtube.com/watch?v=M4GVQZt7MnU), 
[Crabs1_Video](https://www.youtube.com/watch?v=NfpM9BqAaOo), 
[Gymnastics3_Video](https://www.youtube.com/watch?v=fB9S314JZnc),
[Motorcross2_Video](https://www.youtube.com/watch?v=MfveEsYkImw),
[Singer3_Video](https://www.youtube.com/watch?v=iUPZqkkqu7Y),
[Godfather_Video](https://www.youtube.com/watch?v=HKdQpRZGNfw),
[Bag_Video](https://www.youtube.com/watch?v=NiPJs-pKiR0),
[Dinasaur_Video](https://www.youtube.com/watch?v=GEAQluHbf2o),
[Matrix_Video](https://www.youtube.com/watch?v=_l9FxuvHWis),
[Hand_Video](https://www.youtube.com/watch?v=GVbVyvDvRSE),
[Glove_Video](https://www.youtube.com/watch?v=ORnrJzywLCA),
[Ball2_Video](https://www.youtube.com/watch?v=PBUQCcWHN_0),
[Blanket_Video](https://www.youtube.com/watch?v=aznGIj0g88Q),
[Gymnastics1_Video](https://www.youtube.com/watch?v=IQ5LyyScnlM),
[Butterfly_Video](https://www.youtube.com/watch?v=oy2OonlGrIw),
[Motorcross1_Video](https://www.youtube.com/watch?v=U6wSJ6fX8gk),
[Pedestrian_Video](https://www.youtube.com/watch?v=k0NIvzLCdDk),
[Singer2_Video](https://www.youtube.com/watch?v=xuPpytH8Qps),
[Shaking_Video](https://www.youtube.com/watch?v=8oIPuZy4DH0),
[Racing_Video](https://www.youtube.com/watch?v=6Fh-vW4vRHY),
[Handball1_Video](https://www.youtube.com/watch?v=jW7TTJqbjtI),
[Sheep_Video](https://www.youtube.com/watch?v=qDmKJm3usf4),
[Bolt1_Video](https://www.youtube.com/watch?v=EtXuGAsQWJo),
[Fernando_Video](https://www.youtube.com/watch?v=P_nKkfL6DIc),
[Bolt2_Video](https://www.youtube.com/watch?v=CxAYVz9qkUk),
[Book_Video](https://www.youtube.com/watch?v=GJa7q-rmaxA),
[Leaves_Video](https://www.youtube.com/watch?v=oA_Bawv8SaA),
[Fish1_Video](https://www.youtube.com/watch?v=FU1r4JXgfyM),
[Fish2_Video](https://www.youtube.com/watch?v=yQF9VuIh1Yg),
[Tiger_Video](https://www.youtube.com/watch?v=LG3lenIEOC8),
[Wiper_Video](https://www.youtube.com/watch?v=AABuilw6fJk),
[Traffic_Video](https://www.youtube.com/watch?v=_wRB6ZiitcY),
[Crossing_Video](https://www.youtube.com/watch?v=U4_WXqTxTKY),
[Fish3_Video](https://www.youtube.com/watch?v=JReWhzvSkug),
[Ball1_Video](https://www.youtube.com/watch?v=kPrrhZGVl_U),
[Graduate_Video](https://www.youtube.com/watch?v=5Q38SEF3hkA),
[Iceskater_Video](https://www.youtube.com/watch?v=TQrMO25mt50),
[Soldier_Video](https://www.youtube.com/watch?v=Kt-gR67tqm0),
[DroneAcross_Video](https://www.youtube.com/watch?v=vX7WLTvKa_M),
[Soccer2_Video](https://www.youtube.com/watch?v=u9u92BSiSIc),
[DroneFlip_Video](https://www.youtube.com/watch?v=RDRaX0PVc1Y),
[Ants1_Video](https://www.youtube.com/watch?v=ANOM-he1nEs),
[Iceskater_Video](https://www.youtube.com/watch?v=wGYKt8TGkc4),
[Handball2_Video](https://www.youtube.com/watch?v=ZDsIgEdELps),
[Nature_Video](https://www.youtube.com/watch?v=Xmy8diVwD8s),
[Ants3_Video](https://www.youtube.com/watch?v=RXsBGQYeglI),
[Road_Video](https://www.youtube.com/watch?v=inT-VDI9BpY),
[Helicopter_Video](https://www.youtube.com/watch?v=hKW42Wy43CA),
[Girl_Video](https://www.youtube.com/watch?v=FS3ZmQLuUo8),
[Gymnastics2_Video](https://www.youtube.com/watch?v=oRyLZW4GsSo),
[Conduction_Video](https://www.youtube.com/watch?v=80T2zvQhOnU),
[Zebrafish1_Video](https://www.youtube.com/watch?v=VwAr9rnqXQ8),
[Basketball_Video](https://www.youtube.com/watch?v=pDR5n7v82BI),
[Frisbee_Video](https://www.youtube.com/watch?v=lgzWuEBKvSY),
[Car1_Video](https://www.youtube.com/watch?v=sfArINezfzo),
[Birds1_Video](https://www.youtube.com/watch?v=sromxvJbQhs),
[Drone1_Video](https://www.youtube.com/watch?v=RvmgMrItjmI),
[Flamingo_Video](https://www.youtube.com/watch?v=n0HDvwaMJaY).

## Performance Comparison of Visual Trackers in terms of Precision and Success Plots on LaSOT Dataset (Ranking Based on Area under Curve (AUC)) [[LaSOT Dataset]](https://cis.temple.edu/lasot/)[[LaSOT Paper]](https://arxiv.org/abs/1809.07845):
**- Average Performance Comparisons of Visual Tracking Methods:**
<img src="LaSOT%20results/1.png"/> 
<img src="LaSOT%20results/16.png"/> 
**- Attribute-based Performance Comparisons (Eleven attributes including: Illumination Variation (IV), Scale Variation (SV), Occlusion (OCC), Deformation (DEF), Motion Blur (MB), Fast Motion (FM), In-Plane Rotation (IPR), Out-of-Plane Rotation (OPR), Out-of-View (OV), Background Clutter (BC), Low Resolution (LR)):**
<img src="LaSOT%20results/2.png"/> 
<img src="LaSOT%20results/17.png"/> 
<img src="LaSOT%20results/3.png"/> 
<img src="LaSOT%20results/18.png"/> 
<img src="LaSOT%20results/4.png"/> 
<img src="LaSOT%20results/19.png"/> 
<img src="LaSOT%20results/5.png"/> 
<img src="LaSOT%20results/20.png"/> 
<img src="LaSOT%20results/6.png"/> 
<img src="LaSOT%20results/21.png"/> 
<img src="LaSOT%20results/7.png"/> 
<img src="LaSOT%20results/22.png"/> 
<img src="LaSOT%20results/8.png"/> 
<img src="LaSOT%20results/23.png"/> 
<img src="LaSOT%20results/9.png"/> 
<img src="LaSOT%20results/24.png"/> 
<img src="LaSOT%20results/10.png"/> 
<img src="LaSOT%20results/25.png"/> 
<img src="LaSOT%20results/11.png"/> 
<img src="LaSOT%20results/26.png"/> 
<img src="LaSOT%20results/12.png"/> 
<img src="LaSOT%20results/27.png"/> 
<img src="LaSOT%20results/13.png"/> 
<img src="LaSOT%20results/28.png"/> 
<img src="LaSOT%20results/14.png"/> 
<img src="LaSOT%20results/29.png"/> 
<img src="LaSOT%20results/15.png"/> 
<img src="LaSOT%20results/30.png"/> 

## References (Evaluated Visual Tracking Methods):
[1] [C. Ma, J. B. Huang, X. Yang, and M. H. Yang, “Hierarchical convolutional features for visual tracking,” in Proc. IEEE ICCV, 2015, pp. 3074–3082. [HCFT]](https://ieeexplore.ieee.org/document/7410709)<br/>

[2] [M. Danelljan, G. Hager, F. S. Khan, and M. Felsberg, “Convolutional features for correlation filter based visual tracking,” in Proc. IEEE ICCVW, 2016, pp. 621–629. [DeepSRDCF]](https://www.cv-foundation.org/openaccess/content_iccv_2015_workshops/w14/papers/Danelljan_Convolutional_Features_for_ICCV_2015_paper.pdf)<br/>

[3] [M. Danelljan, A. Robinson, F. S. Khan, and M. Felsberg, “Beyond correlation filters: Learning continuous convolution operators for visual tracking,” in Proc. ECCV, vol. 9909 LNCS, 2016, pp. 472–488. [CCOT]](https://link.springer.com/chapter/10.1007/978-3-319-46454-1_29)<br/>

[4] [L. Bertinetto, J. Valmadre, J. F. Henriques, A. Vedaldi, and P. H. Torr, “Fully-convolutional Siamese networks for object tracking,” in Proc. ECCV, 2016, pp. 850–865. [SiamFC]](http://www.robots.ox.ac.uk/~luca/siamese-fc.html)<br/>

[5] [R. Tao, E. Gavves, and A.W. Smeulders, “Siamese instance search for tracking,” in Proc. IEEE CVPR, 2016, pp. 1420–1429. [SINT]](https://ieeexplore.ieee.org/document/7780527)<br/>

[6] [H. Nam and B. Han, “Learning multi-domain convolutional neural networks for visual tracking,” in Proc. IEEE CVPR, 2016, pp. 4293–4302. [MDNet]](https://ieeexplore.ieee.org/document/7780834)<br/>

[7] [Y. Qi, S. Zhang, L. Qin, H. Yao, Q. Huang, J. Lim, and M. H. Yang, “Hedged deep tracking,” in Proc. IEEE CVPR, 2016, pp. 4303–4311. [HDT]](https://ieeexplore.ieee.org/document/7780835)<br/>

[8] [H. Fan and H. Ling, “Parallel tracking and verifying: A framework for real-time and high accuracy visual tracking,” in Proc. IEEE ICCV, 2017, pp. 5487–5495. [PTAV]](http://openaccess.thecvf.com/content_ICCV_2017/papers/Fan_Parallel_Tracking_and_ICCV_2017_paper.pdf)<br/>

[9] [H. Fan and H.Ling, “Parallel tracking and verifying,” IEEE Trans. Image Process., vol. 28, no. 8, pp. 4130–4144, 2019. [PTAV]](https://ieeexplore.ieee.org/document/8667882)<br/>

[10] [Z. Zhu, G. Huang,W. Zou, D. Du, and C. Huang, “UCT: Learning unified convolutional networks for real-time visual tracking,” in Proc. ICCVW, 2018, pp. 1973–1982. [UCT]](http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w28/Zhu_UCT_Learning_Unified_ICCV_2017_paper.pdf)<br/>

[11] [Q. Guo, W. Feng, C. Zhou, R. Huang, L. Wan, and S. Wang, “Learning dynamic Siamese network for visual object tracking,” in Proc. IEEE ICCV, 2017, pp. 1781–1789. [DSiam]](http://openaccess.thecvf.com/content_ICCV_2017/papers/Guo_Learning_Dynamic_Siamese_ICCV_2017_paper.pdf)<br/>

[12] [J. Valmadre, L. Bertinetto, J. Henriques, A. Vedaldi, and P. H. Torr, “End-to-end representation learning for correlation filter based tracking,” in Proc. IEEE CVPR, 2017, pp. 5000–5008. [CFNet]](https://ieeexplore.ieee.org/document/8100014)<br/>

[13] [M. Danelljan, G. Bhat, F. Shahbaz Khan, and M. Felsberg, “ECO: Efficient convolution operators for tracking,” in Proc. IEEE CVPR, 2017, pp. 6931–6939. [ECO]](https://ieeexplore.ieee.org/document/8100216)<br/>

[14] [A. Lukezˇicˇ, T. Voj´ırˇ, L. Cˇ ehovin Zajc, J. Matas, and M. Kristan, “Discriminative correlation filter tracker with channel and spatial reliability,” IJCV, vol. 126, no. 7, pp. 671–688, 2018. [DeepCSRDCF]](https://link.springer.com/article/10.1007/s11263-017-1061-3)<br/>

[15] [T. Zhang, C. Xu, and M. H. Yang, “Multi-task correlation particle filter for robust object tracking,” in Proc. IEEE CVPR, 2017, pp. 4819–4827. [MCPF]](https://ieeexplore.ieee.org/document/8099995)<br/>

[16] [J. Choi, H. J. Chang, S. Yun, T. Fischer, Y. Demiris, and J. Y. Choi, “Attentional correlation filter network for adaptive visual tracking,” in Proc. IEEE CVPR, 2017, pp. 4828–4837. [ACFN]](https://ieeexplore.ieee.org/document/8099996)<br/>

[17] [Q. Wang, J. Gao, J. Xing, M. Zhang, and W. Hu, “DCFNet: Discriminant correlation filters network for visual tracking,” 2017. [Online]. [DCFNet][DCFNet2]](http://arxiv.org/abs/1704.04057)<br/>

[18] [X. Dong and J. Shen, “Triplet loss in Siamese network for object tracking,” in Proc. ECCV, vol. 11217 LNCS, 2018, pp. 472–488. [TripletLoss-CFNet][TripletLoss-SiamFC][TripletLoss-CFNet2]](https://link.springer.com/chapter/10.1007/978-3-030-01261-8_28)<br/>

[19] [G. Bhat, J. Johnander, M. Danelljan, F. S. Khan, and M. Felsberg, “Unveiling the power of deep tracking,” in Proc. ECCV, 2018, pp. 493–509. [UPDT]](https://link.springer.com/chapter/10.1007/978-3-030-01216-8_30)<br/>

[20] [Z. Zhu, Q. Wang, B. Li, W. Wu, J. Yan, and W. Hu, “Distractor-aware Siamese networks for visual object tracking,” in Proc. ECCV, vol. 11213 LNCS, 2018, pp. 103–119. [DaSiamRPN]](https://link.springer.com/chapter/10.1007/978-3-030-01240-3_7)<br/>

[21] [Y. Zhang, L. Wang, J. Qi, D. Wang, M. Feng, and H. Lu, “Structured Siamese network for real-time visual tracking,” in Proc. ECCV, 2018, pp. 355–370. [StructSiam]](https://link.springer.com/chapter/10.1007/978-3-030-01240-3_22)<br/>

[22] [H. Morimitsu, “Multiple context features in Siamese networks for visual object tracking,” in Proc. ECCVW, 2019, pp. 116–131. [Siam-MCF]](https://link.springer.com/chapter/10.1007/978-3-030-11009-3_6)<br/>

[23] [J. Choi, H. J. Chang, T. Fischer, S. Yun, K. Lee, J. Jeong, Y. Demiris, and J. Y. Choi, “Context-aware deep feature compression for high-speed visual tracking,” in Proc. IEEE CVPR, 2018, pp. 479–488. [TRACA]](https://ieeexplore.ieee.org/document/8578155)<br/>

[24] [Y. Song, C. Ma, X. Wu, L. Gong, L. Bao, W. Zuo, C. Shen, R. W. Lau, and M. H. Yang, “VITAL: Visual tracking via adversarial learning,” in Proc. IEEE CVPR, 2018, pp. 8990–8999. [VITAL]](https://ieeexplore.ieee.org/document/8579035)<br/>

[25] [F. Li, C. Tian, W. Zuo, L. Zhang, and M. H. Yang, “Learning spatial-temporal regularized correlation filters for visual tracking,” in Proc. IEEE CVPR, 2018, pp. 4904–4913. [DeepSTRCF]](https://ieeexplore.ieee.org/document/8578613)<br/>

[26] [B. Li, J. Yan, W. Wu, Z. Zhu, and X. Hu, “High performance visual tracking with Siamese region proposal network,” in Proc. IEEE CVPR, 2018, pp. 8971–8980. [SiamRPN]](https://ieeexplore.ieee.org/document/8579033)<br/>

[27] [A. He, C. Luo, X. Tian, andW. Zeng, “A twofold Siamese network for real-time object tracking,” in Proc. IEEE CVPR, 2018, pp. 4834–4843. [SA-Siam]](https://ieeexplore.ieee.org/document/8578606)<br/>

[28] [C. Sun, D. Wang, H. Lu, and M. H. Yang, “Learning spatial-aware regressions for visual tracking,” in Proc. IEEE CVPR, 2018, pp. 8962–8970. [LSART]](https://ieeexplore.ieee.org/document/8579032)<br/>

[29] [C. Sun, D. Wang, H. Lu, and M. H. Yang, “Correlation tracking via joint discrimination and reliability learning,” in Proc. IEEE CVPR, 2018, pp. 489–497. [DRT]](https://ieeexplore.ieee.org/document/8578156)<br/>

[30] [S. Pu, Y. Song, C. Ma, H. Zhang, and M. H. Yang, “Deep attentive tracking via reciprocative learning,” in Proc. NIPS, 2018, pp. 1931–1941. [DAT]](http://papers.nips.cc/paper/7463-deep-attentive-tracking-via-reciprocative-learning)<br/>

[31] [C. Ma, J. B. Huang, X. Yang, and M. H. Yang, “Robust visual tracking via hierarchical convolutional features,” IEEE Trans. Pattern Anal. Mach. Intell., 2018. [HCFTs]](https://ieeexplore.ieee.org/document/8434334)<br/>

[32] [C. Ma, J. B. Huang, X. Yang, and M. H. Yang, “Adaptive correlation filters with long-term and short-term memory for object tracking,” IJCV, vol. 126, no. 8, pp. 771–796, 2018. [LCTdeep]](https://link.springer.com/article/10.1007/s11263-018-1076-4)<br/>

[33] [E. Gundogdu and A. A. Alatan, “Good features to correlate for visual tracking,” IEEE Trans. Image Process., vol. 27, no. 5, pp. 2526–2540, 2018. [CFCF]](https://ieeexplore.ieee.org/document/8291524)<br/>

[34] [H. Fan and H. Ling, “Siamese cascaded region proposal networks for real-time visual tracking,” in Proc. IEEE CVPR, 2019. [C-RPN]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Fan_Siamese_Cascaded_Region_Proposal_Networks_for_Real-Time_Visual_Tracking_CVPR_2019_paper.pdf)<br/>

[35] [J. Gao, T. Zhang, and C. Xu, “Graph convolutional tracking,” in Proc. CVPR, 2019, pp. 4649–4659. [GCT]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Gao_Graph_Convolutional_Tracking_CVPR_2019_paper.pdf)<br/>

[36] [Q. Wang, L. Zhang, L. Bertinetto, W. Hu, and P. H. S. Torr, “Fast online object tracking and segmentation: A unifying approach,” in Proc. IEEE CVPR, 2019. [SiamMask]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Fast_Online_Object_Tracking_and_Segmentation_A_Unifying_Approach_CVPR_2019_paper.pdf)<br/>

[37] [B. Li, W. Wu, Q. Wang, F. Zhang, J. Xing, and J. Yan, “SiamRPN++: Evolution of Siamese visual tracking with very deep networks,” in Proc. IEEE CVPR, 2019. [SiamRPN++]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_SiamRPN_Evolution_of_Siamese_Visual_Tracking_With_Very_Deep_Networks_CVPR_2019_paper.pdf)<br/>

[38] [X. Li, C. Ma, B. Wu, Z. He, and M.-H. Yang, “Target-aware deep tracking,” in Proc. IEEE CVPR, 2019. [TADT]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Li_Target-Aware_Deep_Tracking_CVPR_2019_paper.pdf)<br/>

[39] [K. Dai, D. Wang, H. Lu, C. Sun, and J. Li, “Visual tracking via adaptive spatially-regularized correlation filters,” in Proc. CVPR, 2019, pp. 4670–4679. [ASRCF]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Dai_Visual_Tracking_via_Adaptive_Spatially-Regularized_Correlation_Filters_CVPR_2019_paper.pdf)<br/>

[40] [Z. Zhang and H. Peng, “Deeper and wider Siamese networks for real-time visual tracking,” in Proc. IEEE CVPR, 2019. [SiamDW-SiamRPN][SiamDW-SiamFC]](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Deeper_and_Wider_Siamese_Networks_for_Real-Time_Visual_Tracking_CVPR_2019_paper.pdf)<br/>

[41] [Y. Song, C. Ma, L. Gong, J. Zhang, R. W. Lau, and M. H. Yang, “CREST: Convolutional residual learning for visual tracking,” in Proc. ICCV, 2017, pp. 2574–2583. [CREST][Meta-CREST]](https://ieeexplore.ieee.org/document/8237541)<br/>
